{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import depthai as dai\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RGB camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected cameras:  [<CameraBoardSocket.RGB: 0>, <CameraBoardSocket.LEFT: 1>, <CameraBoardSocket.RIGHT: 2>]\n"
     ]
    }
   ],
   "source": [
    "# Start defining a pipeline\n",
    "pipeline = dai.Pipeline()\n",
    "\n",
    "# Define a source - color camera\n",
    "camRgb = pipeline.createColorCamera()\n",
    "camRgb.setPreviewSize(300, 300)\n",
    "camRgb.setInterleaved(False)\n",
    "camRgb.setColorOrder(dai.ColorCameraProperties.ColorOrder.RGB)\n",
    "\n",
    "# Create output\n",
    "xoutRgb = pipeline.createXLinkOut()\n",
    "xoutRgb.setStreamName(\"rgb\")\n",
    "camRgb.preview.link(xoutRgb.input)\n",
    "\n",
    "# Connect to the device\n",
    "with dai.Device() as device:\n",
    "    # Print out available cameras\n",
    "    print('Connected cameras: ', device.getConnectedCameras())\n",
    "    # Start pipeline\n",
    "    device.startPipeline(pipeline)\n",
    "\n",
    "    # Output queue will be used to get the rgb frames from the output defined above\n",
    "    qRgb = device.getOutputQueue(name=\"rgb\", maxSize=4, blocking=False)\n",
    "\n",
    "    while True:\n",
    "        inRgb = qRgb.get()  # blocking call, will wait until a new data has arrived\n",
    "\n",
    "        # Retrieve 'bgr' (opencv format) frame\n",
    "        frame = inRgb.getCvFrame()\n",
    "        cv.imshow(\"bgr\", frame)\n",
    "\n",
    "        if cv.waitKey(1) == ord('q'):\n",
    "            device.close()\n",
    "            break\n",
    "\n",
    "cv.destroyAllWindows()\n",
    "del pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monochrome camera\n",
    "\n",
    "This example shows how to set up a pipeline that outputs the left and right grayscale camera images, connects over XLink to transfer these to the host real-time, and displays both using OpenCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Pipeline\n",
    "pipeline = dai.Pipeline()\n",
    "\n",
    "# Define a source - two mono (grayscale) cameras\n",
    "camLeft = pipeline.createMonoCamera()\n",
    "camLeft.setBoardSocket(dai.CameraBoardSocket.LEFT)\n",
    "camLeft.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)\n",
    "\n",
    "camRight = pipeline.createMonoCamera()\n",
    "camRight.setBoardSocket(dai.CameraBoardSocket.RIGHT)\n",
    "camRight.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)\n",
    "\n",
    "xoutLeft = pipeline.createXLinkOut()\n",
    "xoutLeft.setStreamName(\"cam_left\")\n",
    "camLeft.out.link(xoutLeft.input)\n",
    "\n",
    "xoutRight = pipeline.createXLinkOut()\n",
    "xoutRight.setStreamName(\"cam_right\")\n",
    "camRight.out.link(xoutRight.input)\n",
    "\n",
    "with dai.Device(pipeline) as device:\n",
    "\n",
    "    qLrft = device.getOutputQueue(name=\"cam_left\", maxSize=4, blocking=False)\n",
    "    qRight = device. getOutputQueue(name=\"cam_right\", maxSize=4, blocking=False)\n",
    "\n",
    "    frameLeft = None \n",
    "    frameRight = None\n",
    "    while True:\n",
    "        inLeft = qLrft.tryGet()\n",
    "        inRight = qRight.tryGet()\n",
    "\n",
    "        if inLeft is not None:\n",
    "            frameLeft = inLeft.getCvFrame()\n",
    "        if inRight is not None:\n",
    "            frameRight = inRight.getCvFrame()\n",
    "        \n",
    "        if frameLeft is not None:\n",
    "            cv.imshow(\"left\", frameLeft)\n",
    "        if frameRight is not None:\n",
    "            cv.imshow(\"right\", frameRight)\n",
    "\n",
    "        '''\n",
    "        Best practice to close camera and OpenCV\n",
    "        '''\n",
    "        if cv.waitKey(1) == ord('q'):\n",
    "            device.close()\n",
    "            break\n",
    "\n",
    "cv.destroyAllWindows()\n",
    "del pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Depth preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional flags\n",
    "extend_disparity = False\n",
    "subpixel = False\n",
    "lr_check = False\n",
    "\n",
    "# Start defining a pipeline\n",
    "pipeline = dai.Pipeline()\n",
    "\n",
    "# Define a source - two momo cams\n",
    "camLeft = pipeline.createMonoCamera()\n",
    "camLeft.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)\n",
    "camLeft.setBoardSocket(dai.CameraBoardSocket.LEFT)\n",
    "\n",
    "camRight = pipeline.createMonoCamera()\n",
    "camRight.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)\n",
    "camRight.setBoardSocket(dai.CameraBoardSocket.RIGHT)\n",
    "\n",
    "# Create a node that will produce the depth map\n",
    "depth = pipeline.createStereoDepth()\n",
    "depth.setConfidenceThreshold(200)\n",
    "\n",
    "# Set up median filter kernel\n",
    "median = dai.StereoDepthProperties.MedianFilter.KERNEL_7x7\n",
    "depth.setMedianFilter(median)\n",
    "\n",
    "# Left-Right check\n",
    "depth.setLeftRightCheck(lr_check)\n",
    "\n",
    "# Normal disparity values range from 0..95 wil be used for normalisation\n",
    "max_disparity = 95\n",
    "\n",
    "if extend_disparity:\n",
    "    max_disparity = max_disparity * 2   # double the range\n",
    "if subpixel:\n",
    "    max_disparity = max_disparity * 32  # 5 fractional bits (2^5)\n",
    "depth.setExtendedDisparity(extend_disparity)\n",
    "depth.setSubpixel(subpixel)\n",
    "\n",
    "# When we get disparity to the host, we will multiply all values with the multiplier for better visualisation\n",
    "multiplier = 255 / max_disparity\n",
    "    \n",
    "camLeft.out.link(depth.left)\n",
    "camRight.out.link(depth.right)\n",
    "\n",
    "# Create output\n",
    "xout = pipeline.createXLinkOut()\n",
    "xout.setStreamName(\"disparity\")\n",
    "depth.disparity.link(xout.input)\n",
    "\n",
    "with dai.Device(pipeline) as device:\n",
    "    qDisparity = device.getOutputQueue(name=\"disparity\", maxSize=4, blocking=False)\n",
    "    while True:\n",
    "        inDepth = qDisparity.get()\n",
    "        \n",
    "        frameDepth = inDepth.getFrame()\n",
    "        frameDepth = (frameDepth * multiplier).astype(np.uint8)\n",
    "        frameDepth = cv.applyColorMap(frameDepth, cv.COLORMAP_JET)\n",
    "                \n",
    "        cv.imshow(\"disparity\", frameDepth)\n",
    "        \n",
    "        if cv.waitKey(1) == ord('q'):\n",
    "            device.close()\n",
    "            break\n",
    "\n",
    "cv.destroyAllWindows()\n",
    "del pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
